{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/Me00a24lXvzC0G6VKEP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sadiagul914/AI201/blob/main/Langchain_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMUeN_6hYzPG",
        "outputId": "6906f55a-7e74-4d4b-bc0f-ac5022b3b551",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.8.3)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.3.28)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (2.10.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.2.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "6dvf-EPUZMfU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q3uAs-JeZMlj",
        "outputId": "ffe5998f-e7f4-4027-95da-67c64be29ebe"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.28)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n"
      ],
      "metadata": {
        "id": "gfWAA1MtZMsA",
        "collapsed": true
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part1: Gemini Features like temperature, tokens, max retries etc"
      ],
      "metadata": {
        "id": "1C35uXevPPkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2) # gemini features\n",
        "llm.invoke(\"HelloWorld!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YlnZFJmDRAkd",
        "outputId": "3f003886-07aa-4985-e292-80bd6a4acbf1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello World!\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-ddaf4dc3-0ae6-45e5-a3fb-7a2cd6949c45-0', usage_metadata={'input_tokens': 3, 'output_tokens': 4, 'total_tokens': 7, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part2:More prompt templates"
      ],
      "metadata": {
        "id": "OP9f8-iwHE-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the LLM chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Run the chain with a sample question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-4G-WibC9Jj",
        "outputId": "e9b53f7d-4b0c-4655-997a-445a4d45847b",
        "collapsed": true
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework designed to simplify the development of applications using large language models (LLMs). It provides a modular and extensible set of tools, components, and interfaces to connect LLMs with other sources of computation or data, allowing for more complex and useful applications.\n",
            "\n",
            "Here's a breakdown of what that means and why it's important:\n",
            "\n",
            "* **Connecting LLMs with other data:**  LLMs are powerful, but their knowledge is limited to their training data. LangChain lets you connect them to external data sources (like files, APIs, or databases) so they can access up-to-date information and personalize responses.\n",
            "\n",
            "* **Enabling complex workflows:**  LangChain allows you to chain together multiple LLM calls or other operations (like making API calls or querying a database) to create sophisticated applications. For example, you could create a chatbot that accesses product information from a database, summarizes it using an LLM, and then uses another LLM to generate a personalized sales pitch.\n",
            "\n",
            "* **Modular and extensible:** LangChain offers a range of pre-built components for common tasks, but it's also designed to be easily extended and customized. You can create your own modules or integrate with other libraries to tailor the framework to your specific needs.\n",
            "\n",
            "* **Simplified development:** By providing a standardized set of tools and interfaces, LangChain reduces the boilerplate code required to build LLM-powered applications. This makes development faster and easier, allowing developers to focus on the unique aspects of their projects.\n",
            "\n",
            "**Key features and concepts within LangChain include:**\n",
            "\n",
            "* **Models:** Interfaces for interacting with various LLMs, chat models, and embeddings models.\n",
            "* **Prompts:** Tools for constructing and managing prompts, including templating and chain-of-thought prompting.\n",
            "* **Chains:** Sequences of calls to LLMs or other utilities.\n",
            "* **Indexes:** Structures for organizing and accessing external data efficiently.\n",
            "* **Agents:** Components that decide which actions to take based on LLM outputs, enabling more autonomous behavior.\n",
            "* **Memory:** Mechanisms for persisting state between chain executions, allowing for context-aware conversations.\n",
            "* **Callbacks:** Functions that can be executed at various points in a chain, enabling monitoring and customization.\n",
            "\n",
            "\n",
            "In short, LangChain empowers developers to build more powerful and sophisticated applications by leveraging the strengths of LLMs while mitigating their limitations.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1FB6QVDVVU",
        "outputId": "7d172350-f9fc-403e-a55c-1e92e380829b",
        "collapsed": true
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part3: Adding Memory"
      ],
      "metadata": {
        "id": "4-u5nhiIk8OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create the conversation chain with memory\n",
        "conversation_chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Simulate a multi-turn conversation\n",
        "user_inputs = [\n",
        "    \"Hello, can you explain what LangChain is?\",\n",
        "    \"How does memory work in LangChain?\",\n",
        "    \"Can LangChain be used with Google Gemini for applications?\"\n",
        "]\n",
        "\n",
        "for user_input in user_inputs:\n",
        "    response = conversation_chain.run(user_input)\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"Assistant: {response}\\n\")\n",
        "\n",
        "# Display the stored memory for debugging\n",
        "print(\"Conversation Memory:\")\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "feOyQ4_rXCs8",
        "outputId": "a64889af-693a-48ab-dd35-5a82714a90cf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello, can you explain what LangChain is?\n",
            "Assistant: Hello! LangChain is a framework specifically designed for developing applications powered by language models. Think of it as a toolbox filled with components that make it easier to build sophisticated applications that go beyond just simple chatbots.  It's built around the idea of \"chains,\" which are sequences of calls to language models or other utilities. These chains can be quite complex, allowing for a wide range of functionalities.\n",
            "\n",
            "A key feature of LangChain is its management of prompts.  It provides a structured way to create and manage prompts, including templating and versioning.  This is crucial because the quality of the output from a language model heavily depends on the quality of the prompt.  LangChain helps you keep track of your prompts and experiment with different variations systematically.\n",
            "\n",
            "Furthermore, LangChain facilitates connecting language models to external data sources.  Imagine you want to build an application that can answer questions about specific documents or interact with a live database. LangChain provides the tools to make these connections seamlessly. It handles the retrieval of relevant information and integrates it into the language model's workflow.\n",
            "\n",
            "Another important aspect is its support for \"agents.\"  Agents allow language models to interact with their environment. This could involve searching the web, accessing APIs, or even running code.  LangChain provides a framework for defining and managing these agent interactions, enabling the development of more dynamic and interactive applications.\n",
            "\n",
            "Specific examples of what you can build with LangChain include chatbots, generative question-answering systems, summarization tools, and even agents that can perform actions based on natural language instructions.  It's a rapidly evolving framework with an active community, so new features and capabilities are constantly being added.  It's primarily implemented in Python and JavaScript, making it accessible to a broad range of developers.\n",
            "\n",
            "\n",
            "User: How does memory work in LangChain?\n",
            "Assistant: Memory in LangChain is a crucial component for enabling more sophisticated and interactive applications.  It allows language models to retain information from previous interactions within a conversation, providing context and continuity.  Without memory, each interaction would be treated in isolation, leading to a less engaging and potentially confusing user experience.\n",
            "\n",
            "LangChain offers several different types of memory implementations, each suited to different use cases:\n",
            "\n",
            "* **ConversationBufferMemory:** This is a simple memory implementation that stores all previous messages in a conversation as strings.  It's easy to use and understand, but it can become inefficient for long conversations as the entire history is stored.\n",
            "\n",
            "* **ConversationBufferWindowMemory:** Similar to `ConversationBufferMemory`, but it only keeps track of a specified number of recent interactions. This helps manage the amount of stored information and improves efficiency in longer conversations.  You define the window size, determining how many previous turns are remembered.\n",
            "\n",
            "* **ConversationSummaryMemory:** This type of memory summarizes the conversation history, providing a concise representation of what has been discussed. It's useful for keeping track of the overall topic and direction of the conversation, even if specific details are lost.  Different summarization strategies can be employed.\n",
            "\n",
            "* **ConversationEntityMemory:**  This memory focuses on tracking specific entities mentioned in the conversation.  For example, if you're discussing a project with a deadline, the `ConversationEntityMemory` would store information about the project name and the deadline.  This allows the language model to easily access and refer to these key pieces of information.\n",
            "\n",
            "* **CombinedMemory:**  LangChain allows you to combine different memory types to create more complex memory strategies.  For example, you could combine `ConversationBufferWindowMemory` with `ConversationEntityMemory` to retain recent messages while also tracking specific entities.\n",
            "\n",
            "\n",
            "Beyond these basic types, LangChain provides a modular framework for defining custom memory implementations. This flexibility allows developers to tailor the memory functionality to the specific requirements of their applications. You can even integrate with external databases or vector stores to persist memory across sessions.\n",
            "\n",
            "Choosing the right memory implementation depends on the specific application.  For simple chatbots, `ConversationBufferMemory` might suffice. For more complex applications requiring context awareness and long-term memory, a more sophisticated approach using `ConversationSummaryMemory`, `ConversationEntityMemory`, or a custom implementation might be necessary.\n",
            "\n",
            "\n",
            "User: Can LangChain be used with Google Gemini for applications?\n",
            "Assistant: While LangChain is designed to be flexible and integrate with various language models, I don't currently have specific information confirming direct integration with Google Gemini.  LangChain's core functionality revolves around providing a standardized interface for interacting with different language models through components like LLMs, ChatModels, etc.  Therefore, integration with Gemini would likely involve implementing the necessary interfaces and wrappers to allow LangChain to communicate with Gemini's API.\n",
            "\n",
            "Given the rapidly evolving nature of the language model landscape, it's possible that such integration is already underway or planned within the LangChain community.  Checking the official LangChain documentation, GitHub repository, or community forums would be the best way to find the most up-to-date information on Gemini integration.  Looking for examples or discussions related to \"LangChain Google Gemini\" in these resources could provide valuable insights.\n",
            "\n",
            "\n",
            "Conversation Memory:\n",
            "Human: Hello, can you explain what LangChain is?\n",
            "AI: Hello! LangChain is a framework specifically designed for developing applications powered by language models. Think of it as a toolbox filled with components that make it easier to build sophisticated applications that go beyond just simple chatbots.  It's built around the idea of \"chains,\" which are sequences of calls to language models or other utilities. These chains can be quite complex, allowing for a wide range of functionalities.\n",
            "\n",
            "A key feature of LangChain is its management of prompts.  It provides a structured way to create and manage prompts, including templating and versioning.  This is crucial because the quality of the output from a language model heavily depends on the quality of the prompt.  LangChain helps you keep track of your prompts and experiment with different variations systematically.\n",
            "\n",
            "Furthermore, LangChain facilitates connecting language models to external data sources.  Imagine you want to build an application that can answer questions about specific documents or interact with a live database. LangChain provides the tools to make these connections seamlessly. It handles the retrieval of relevant information and integrates it into the language model's workflow.\n",
            "\n",
            "Another important aspect is its support for \"agents.\"  Agents allow language models to interact with their environment. This could involve searching the web, accessing APIs, or even running code.  LangChain provides a framework for defining and managing these agent interactions, enabling the development of more dynamic and interactive applications.\n",
            "\n",
            "Specific examples of what you can build with LangChain include chatbots, generative question-answering systems, summarization tools, and even agents that can perform actions based on natural language instructions.  It's a rapidly evolving framework with an active community, so new features and capabilities are constantly being added.  It's primarily implemented in Python and JavaScript, making it accessible to a broad range of developers.\n",
            "\n",
            "Human: How does memory work in LangChain?\n",
            "AI: Memory in LangChain is a crucial component for enabling more sophisticated and interactive applications.  It allows language models to retain information from previous interactions within a conversation, providing context and continuity.  Without memory, each interaction would be treated in isolation, leading to a less engaging and potentially confusing user experience.\n",
            "\n",
            "LangChain offers several different types of memory implementations, each suited to different use cases:\n",
            "\n",
            "* **ConversationBufferMemory:** This is a simple memory implementation that stores all previous messages in a conversation as strings.  It's easy to use and understand, but it can become inefficient for long conversations as the entire history is stored.\n",
            "\n",
            "* **ConversationBufferWindowMemory:** Similar to `ConversationBufferMemory`, but it only keeps track of a specified number of recent interactions. This helps manage the amount of stored information and improves efficiency in longer conversations.  You define the window size, determining how many previous turns are remembered.\n",
            "\n",
            "* **ConversationSummaryMemory:** This type of memory summarizes the conversation history, providing a concise representation of what has been discussed. It's useful for keeping track of the overall topic and direction of the conversation, even if specific details are lost.  Different summarization strategies can be employed.\n",
            "\n",
            "* **ConversationEntityMemory:**  This memory focuses on tracking specific entities mentioned in the conversation.  For example, if you're discussing a project with a deadline, the `ConversationEntityMemory` would store information about the project name and the deadline.  This allows the language model to easily access and refer to these key pieces of information.\n",
            "\n",
            "* **CombinedMemory:**  LangChain allows you to combine different memory types to create more complex memory strategies.  For example, you could combine `ConversationBufferWindowMemory` with `ConversationEntityMemory` to retain recent messages while also tracking specific entities.\n",
            "\n",
            "\n",
            "Beyond these basic types, LangChain provides a modular framework for defining custom memory implementations. This flexibility allows developers to tailor the memory functionality to the specific requirements of their applications. You can even integrate with external databases or vector stores to persist memory across sessions.\n",
            "\n",
            "Choosing the right memory implementation depends on the specific application.  For simple chatbots, `ConversationBufferMemory` might suffice. For more complex applications requiring context awareness and long-term memory, a more sophisticated approach using `ConversationSummaryMemory`, `ConversationEntityMemory`, or a custom implementation might be necessary.\n",
            "\n",
            "Human: Can LangChain be used with Google Gemini for applications?\n",
            "AI: While LangChain is designed to be flexible and integrate with various language models, I don't currently have specific information confirming direct integration with Google Gemini.  LangChain's core functionality revolves around providing a standardized interface for interacting with different language models through components like LLMs, ChatModels, etc.  Therefore, integration with Gemini would likely involve implementing the necessary interfaces and wrappers to allow LangChain to communicate with Gemini's API.\n",
            "\n",
            "Given the rapidly evolving nature of the language model landscape, it's possible that such integration is already underway or planned within the LangChain community.  Checking the official LangChain documentation, GitHub repository, or community forums would be the best way to find the most up-to-date information on Gemini integration.  Looking for examples or discussions related to \"LangChain Google Gemini\" in these resources could provide valuable insights.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part4: Tool Calling"
      ],
      "metadata": {
        "id": "zL-eIwNoG8ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool creation\n",
        "\n",
        "# Tool binding\n",
        "\n",
        "# Tool calling\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "   \"\"\"Multiply two numbers.\"\"\"\n",
        "   return a * b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "query = \"What is 3 * 12?\"\n",
        "\n",
        "llm_with_tools.invoke(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFyd7dfDGndG",
        "outputId": "07056e27-c5f6-4af9-8245-11e292880147",
        "collapsed": true
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "Multiply two numbers.\n",
            "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 12.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b50c3c14-b29b-417f-8578-fcbb4143173b-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '52f47ba9-7bdb-494e-b91b-624ab574a252', 'type': 'tool_call'}], usage_metadata={'input_tokens': 104, 'output_tokens': 3, 'total_tokens': 107, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional code to call weather API"
      ],
      "metadata": {
        "id": "GAozwZ-zmgl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai  import ChatGoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Define tools\n",
        "# Tool 1: Database query\n",
        "def run_database_query(query):\n",
        "    # Simulate querying a database\n",
        "    return f\"Database result for: {query}\"\n",
        "\n",
        "database_tool = Tool(\n",
        "    name=\"DatabaseQuery\",\n",
        "    func=run_database_query,\n",
        "    description=\"Executes database queries. Provide a query to fetch the result.\"\n",
        ")\n",
        "\n",
        "# Tool 2: Weather API\n",
        "def get_weather(location):\n",
        "    # Simulate fetching weather data\n",
        "    return f\"The weather in {location} is sunny and 75Â°F.\"\n",
        "\n",
        "weather_tool = Tool(\n",
        "    name=\"WeatherAPI\",\n",
        "    func=get_weather,\n",
        "    description=\"Gets the current weather for a given location.\"\n",
        ")\n",
        "\n",
        "tools = [database_tool, weather_tool]\n",
        "\n",
        "# Step 3: Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Simulate a multi-turn conversation\n",
        "print(\"\\n--- Starting Conversation ---\\n\")\n",
        "user_inputs = [\n",
        "    \"What is the latest sales data from the database?\",\n",
        "    \"Can you tell me the weather in New York City?\"\n",
        "]\n",
        "\n",
        "for user_input in user_inputs:\n",
        "    response = conversation_chain.run(user_input)\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"Assistant: {response}\\n\")\n",
        "\n",
        "# Step 6: Review conversation memory\n",
        "print(\"\\n--- Conversation Memory ---\")\n",
        "print(memory.load_memory_variables({}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_AcnkcZug8a5",
        "outputId": "a2493fe1-7a00-4de6-910c-112b648e7d83"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Conversation ---\n",
            "\n",
            "User: What is the latest sales data from the database?\n",
            "Assistant: As I mentioned before, I do not have access to any external databases or private information, including sales data.  My responses are based on the information I was trained on, and I cannot directly interact with databases containing real-time or private data.  You would need to query the relevant database directly to retrieve that information.\n",
            "\n",
            "\n",
            "User: Can you tell me the weather in New York City?\n",
            "Assistant: I do not have access to real-time information, including current weather conditions.  To get the latest weather in New York City, I would suggest checking a dedicated weather app or website. These resources are typically connected to live meteorological data and can provide accurate and up-to-date forecasts.\n",
            "\n",
            "\n",
            "\n",
            "--- Conversation Memory ---\n",
            "{'chat_history': []}\n"
          ]
        }
      ]
    }
  ]
}